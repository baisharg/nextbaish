{
  "header": {
    "nav": {
      "about": "About",
      "activities": "Programs",
      "research": "Research",
      "resources": "Resources",
      "contact": "Contact"
    },
    "cta": "Join Us",
    "language": "Language",
    "languages": {
      "en": "English",
      "es": "Español"
    },
    "menu": "Menu",
    "closeMenu": "Close menu",
    "openMenu": "Open menu"
  },
  "footer": {
    "copyright": "All rights reserved.",
    "nav": {
      "about": "About",
      "activities": "Programs",
      "research": "Research",
      "resources": "Resources",
      "contact": "Contact",
      "getInvolved": "Get Involved",
      "privacyPolicy": "Privacy Policy"
    }
  },
  "substack": {
    "eyebrow": "Newsletter",
    "title": "Join our Mailing List",
    "description": "Stay updated with our activities, events, and opportunities.",
    "placeholder": "you@example.com",
    "button": "Subscribe",
    "disclaimer": "We'll only email when something important is happening.",
    "success": "You're subscribed! Check your inbox for the welcome email.",
    "confirm": "Almost done — please confirm the subscription from your inbox.",
    "genericError": "Something went wrong. Please try again.",
    "networkError": "Connection failed. Please check your internet and retry."
  },
  "home": {
    "breadcrumb": {
      "home": "Home"
    },
    "mission": {
      "eyebrow": "Mission",
      "title": "Ensuring AI Benefits Humanity",
      "paragraph1": "As artificial intelligence models advance in capabilities{footnote1}, we expect them to have an increasingly profound impact on our society{footnote2}. It is essential that this impact is positive, and that the decisions made by these systems are transparent, reliable, and accountable{footnote3} to the people affected by them.",
      "paragraph2": "We believe that reducing the risks associated with advanced AI models{footnote4} is one of the most important challenges of our time. We also believe it is an open and exciting problem{footnote5}, with ample opportunities for more researchers to advance in this field{footnote6}.",
      "paragraph3": "BAISH's mission is to support students in entering this field and conducting research on this topic.",
      "cta": "Get Involved"
    },
    "events": {
      "eyebrow": "Upcoming Events",
      "title": "Join our community workshops and meetups",
      "description": "Stay current on AI safety discussions, research sprints, and collaboration spaces.",
      "calendarPlaceholder": "Calendar loads once it's almost in view to keep the initial load quick.",
      "subscribe": "Subscribe to Calendar"
    },
    "activities": {
      "title": "Our Programs",
      "description": "Explore recurring programs designed to grow the AI safety community.",
      "items": {
        "fundamentals": {
          "eyebrow": "Course",
          "title": "AI Safety Fundamentals",
          "description": "Inverted classroom course with weekly discussions on AI alignment and final mini-project.",
          "schedule": "Fridays · 2:30pm",
          "duration": "13 weeks"
        },
        "workshop": {
          "eyebrow": "Workshop",
          "title": "AIS Research Workshop",
          "description": "Technical workshop to replicate recent AI Safety research through programming.",
          "schedule": "Tuesdays · 2:30pm",
          "duration": "2.5 hrs"
        },
        "reading": {
          "eyebrow": "Presentations",
          "title": "Paper Presentations Club",
          "description": "Full paper presentations by community members with YouTube recordings.",
          "schedule": "Alternate Thursdays · 3pm",
          "duration": "2 hrs"
        }
      },
      "joinNow": "Join now",
      "joinTelegram": "Join Telegram",
      "learnMore": "Learn more"
    },
    "aisar": {
      "eyebrow": "Sister Project",
      "title": "AISAR Scholarships",
      "description": "In-person part-time program for advanced students to work on AI Safety research projects. $600/month stipend + computing budget. Topics include interpretability, evaluations, and oversight.",
      "duration": "6 months",
      "commitment": "20 hrs/week",
      "visitWebsite": "Visit Website"
    },
    "resources": {
      "title": "Resources",
      "description": "Dive deeper into AI safety with curated readings and tools.",
      "items": {
        "reading": {
          "eyebrow": "Resources",
          "title": "Starter Reading List",
          "description": "Foundational essays and papers to get oriented in AI alignment research.",
          "meta": "15+ papers"
        },
        "toolkit": {
          "eyebrow": "Resources",
          "title": "Research Toolkit",
          "description": "Code repositories, benchmarks, and experiment templates for AI safety research.",
          "meta": "Tools & code"
        },
        "handbook": {
          "eyebrow": "Resources",
          "title": "Community Handbook",
          "description": "Guidelines for hosting meetups and facilitating productive discussions.",
          "meta": "Guide"
        }
      },
      "viewResource": "View resource",
      "learnMore": "Learn more"
    },
    "getInvolved": {
      "communityEyebrow": "Community",
      "communityTitle": "Join our Community",
      "communityDescription": "Connect with fellow students interested in AI safety.",
      "telegramCta": "AI Safety Argentina",
      "telegramMembers": "190+ members",
      "whatsappCta": "BAISH Community",
      "whatsappMembers": "140+ members"
    }
  },
  "about": {
    "breadcrumb": {
      "home": "Home",
      "current": "About AI Safety"
    },
    "title": "Understanding AI Safety",
    "coreConcepts": {
      "whatIsAiSafety": {
        "title": "What is AI Safety?",
        "content": "AI Safety is a research field focused on ensuring that advanced artificial intelligence systems remain beneficial, aligned with human values, and under human control as they become more capable. It encompasses technical research areas like alignment, interpretability, and robustness, as well as governance considerations about how AI systems should be developed and deployed."
      },
      "whyItMatters": {
        "title": "Why It Matters",
        "content": "As AI systems become more powerful and autonomous, they may develop capabilities that could lead to unintended consequences if not properly designed and controlled. The stakes are high: advanced AI could help solve humanity's greatest challenges, but also poses significant risks if developed without adequate safety measures. The field aims to maximize the benefits while minimizing potential harms."
      },
      "risks": {
        "title": "Key Risks & Challenges",
        "alignment": {
          "title": "Alignment Problem",
          "description": "Ensuring AI systems pursue goals aligned with human values and intentions, even as they become more capable."
        },
        "interpretability": {
          "title": "Interpretability",
          "description": "Developing techniques to understand how AI systems make decisions and represent knowledge."
        },
        "robustness": {
          "title": "Robustness",
          "description": "Creating systems that behave safely even when deployed in new environments or facing unexpected situations."
        },
        "powerSeeking": {
          "title": "Power-seeking Behavior",
          "description": "Preventing AI systems from developing instrumental goals that conflict with human welfare."
        },
        "coordination": {
          "title": "Coordination Challenges",
          "description": "Ensuring that safety standards are maintained across all major AI development efforts globally."
        }
      }
    },
    "ourApproach": {
      "title": "Our Approach",
      "focusAreas": {
        "title": "Focus Areas",
        "intro": "At BAISH - Buenos Aires AI Safety Hub, we focus on several key areas within AI safety research:",
        "items": [
          "Chain of Thought interpretability",
          "LLM evaluations",
          "Mechanistic interpretability of neural networks"
        ]
      },
      "contribution": {
        "title": "Our Contribution",
        "intro": "We contribute to the field through:",
        "items": [
          "Supporting student research projects",
          "Building a regional community of AI safety researchers",
          "Organizing workshops, training programs, and hackathons",
          "Mentoring students interested in AI safety careers"
        ]
      }
    },
    "team": {
      "title": "Our Core Team",
      "meetTheTeam": "Meet the Team",
      "cofoundersTitle": "Cofounders",
      "leadershiptTitle": "Leadership",
      "roles": {
        "coDirector": "Co-founding Director",
        "commDirector": "Communications Director",
        "headOfStrategy": "Head of Strategy",
        "advisor": "Advisor"
      },
      "volunteersTitle": "Volunteers",
      "volunteerRoles": {
        "asfFacilitator": "AI Safety Fundamentals Facilitator",
        "aisWorkshopFacilitator": "AIS Workshop Facilitator",
        "programAssistant": "Program Assistant"
      },
      "bios": {
        "eitan": "Eitan is a co-founding director of BAISH with expertise in AI safety research and interpretability. He leads technical research initiatives and helps coordinate the community's research activities.",
        "luca": "Luca is a co-founding director of BAISH focused on building the AI safety research community in Buenos Aires. He brings experience in machine learning and is passionate about making AI safety research more accessible."
      }
    },
    "support": {
      "title": "Supported By",
      "description": "BAISH is supported by:",
      "openPhilanthropy": {
        "name": "Open Philanthropy",
        "description": "Open Philanthropy's mission is to give as effectively as possible in order to help others as much as possible."
      },
      "kairos": {
        "name": "Kairos",
        "program": "Pathfinder Program",
        "description": "Kairos supports AI safety field-building through their Pathfinder program, which helps accelerate promising initiatives."
      }
    }
  },
  "activities": {
    "breadcrumb": {
      "home": "Home",
      "current": "Programs"
    },
    "title": "Our Programs",
    "description": "Join our community and participate in AI safety research and learning",
    "subscribeCalendar": "Subscribe to Events Calendar",
    "agiSafety": {
      "title": "AI Safety Fundamentals",
      "status": "Currently Active",
      "description": "The activity consists of discussing and following up in person on the contents of BlueDot's AI Alignment course, which must be completed asynchronously.",
      "overview": "The (unofficial) course uses a flipped classroom format. Each week, participants must come with the content read/watched (2-3 hours), and on Fridays from 2:30 PM to 5:00 PM we meet in person.",
      "whatToExpect": "Course Format",
      "expectations": [
        "13 total sessions: 9 conceptual + 4 project-based",
        "Conceptual sessions: Discussions on content with pre-designed dynamics",
        "Project sessions: Personal mini-project with mentorship",
        "2-3 hours weekly of asynchronous content",
        "2.5 hours weekly in-person (Fridays 2:30-5:00 PM)",
        "Completion certificate (requires 7/9 sessions + final project)"
      ],
      "whoWeSeek": "Who We're Looking For",
      "seekCriteria": [
        "You're interested in addressing concepts like vulnerabilities in AI systems or existential risks",
        "You want to explore what opportunities exist in the field and your next steps",
        "You're willing to participate in open discussions",
        "You have a good level of English",
        "You can dedicate yourself to the in-person course (approximately 2.5 hours per week)",
        "You can dedicate yourself to asynchronous activities and review material (approximately 3 hours per week)"
      ],
      "programDetails": "Program Details",
      "duration": "13 weeks",
      "fellowshipPeriod": "Fridays 2:30-5:00 PM",
      "viewCurriculum": "View Curriculum",
      "applyNow": "Apply Now"
    },
    "aisWorkshop": {
      "title": "AIS Research Workshop",
      "schedule": "Every Tuesday @ 2:30 PM",
      "description": "The activity consists of discussing and replicating (through programming) recent AI Safety research in person to acquire the technical skills necessary to conduct independent research.",
      "overview": "This workshop takes place on Tuesdays from 2:30 PM to 5:00 PM and is designed for students who want to develop practical technical skills in AI safety.",
      "whoWeSeek": "Who We're Looking For",
      "seekCriteria": [
        "You can dedicate at least 5 hours per week (2 hrs in-person + 3 hrs asynchronous)",
        "You have a good level of English",
        "You're willing to participate in open discussions",
        "You're interested in acquiring technical skills in ML and Safety",
        "You want to enter the field and take your first steps as a researcher"
      ],
      "formatTitle": "Workshop Format",
      "format": [
        "2.5-hour in-person sessions on Tuesdays",
        "Discussion of recent AI Safety papers",
        "Practical implementation by programming the discussed methods",
        "Asynchronous work to familiarize yourself with materials (3 hrs/week)",
        "Mentorship to develop independent research skills"
      ],
      "nextSession": "Next Session",
      "date": "Check calendar",
      "time": "2:30 PM - 5:00 PM",
      "location": "Pabellon 0+inf, Ciudad Universitaria",
      "topic": "To be defined weekly",
      "joinTelegram": "Join Telegram for Updates",
      "joinWhatsapp": "Join WhatsApp",
      "applyNow": "Apply Now"
    },
    "paperReading": {
      "title": "Paper Presentations Club",
      "schedule": "Alternate Thursdays @ 3:00 PM",
      "description": "The Paper Reading Club presents full AI safety papers presented by community members. Each session includes an in-depth presentation followed by discussion.",
      "overview": "Presentations are recorded and uploaded to our YouTube channel so the community can review them later.",
      "selectionCriteriaTitle": "What to Expect",
      "criteria": [
        "Full paper presentations by community members",
        "In-depth analysis of methods and results",
        "Open discussion about implications",
        "Recordings available on YouTube"
      ],
      "sessionFormatTitle": "Session Format",
      "format": [
        "Full paper presentation (30-40 minutes)",
        "Open discussion and Q&A (30-40 minutes)",
        "Recorded session uploaded to YouTube",
        "Telegram chat for coordination and ongoing discussion"
      ],
      "nextSession": "Next Paper Session",
      "paper": "Check Telegram or calendar",
      "discussionLead": "Community member",
      "accessList": "Watch recordings on YouTube",
      "date": "Check calendar",
      "time": "3:00 PM - 5:00 PM",
      "location": "Pabellon 0+inf, Ciudad Universitaria",
      "youtubeChannel": "YouTube Channel",
      "telegramGroup": "Telegram Group"
    },
    "common": {
      "duration": "Duration:",
      "startDate": "Start Date:",
      "endDate": "End Date:",
      "applicationDeadline": "Application Deadline:",
      "location": "Location:",
      "instructors": "Instructors:",
      "fellowshipPeriod": "Fellowship Period:",
      "date": "Date:",
      "time": "Time:",
      "topic": "Topic:",
      "facilitator": "Facilitator:",
      "paper": "Paper:",
      "discussionLead": "Discussion Lead:"
    },
    "gallery": {
      "eyebrow": "Community",
      "title": "Past Events",
      "description": "Moments from our recent gatherings and activities"
    },
    "sisterProjects": {
      "title": "Sister Projects",
      "description": "Collaborative initiatives advancing AI safety across Latin America"
    },
    "lanais": {
      "eyebrow": "Sister Project",
      "title": "LANAIS",
      "subtitle": "Latin American Network for AI Safety",
      "description": "Connecting researchers and policy-makers for safe artificial intelligence in Latin America. LANAIS maintains a directory of AI safety experts across the region and operates in English, Spanish, and Portuguese.",
      "visitWebsite": "Visit Website"
    },
    "fair": {
      "eyebrow": "Sister Project",
      "title": "FAIR",
      "subtitle": "Frontier Artificial Intelligence Research",
      "description": "AI safety research organization at University of Buenos Aires advancing frontier AI safety through high-impact research. Focus areas include AI alignment, existential risks, governance, and systems evaluations.",
      "visitWebsite": "Visit Website"
    }
  },
  "contact": {
    "breadcrumb": {
      "home": "Home",
      "current": "Contact"
    },
    "title": "Contact Us",
    "description": "Get in touch with our team and join our community",
    "linkText": {
      "resourcesPage": "Resources page"
    },
    "cards": {
      "telegram": {
        "title": "Telegram",
        "description": "Join our community channel for discussions and updates:"
      },
      "location": {
        "eyebrow": "Visit Us",
        "title": "Location",
        "description": "We're based at the Department of Computer Science:"
      },
      "social": {
        "eyebrow": "Social",
        "title": "Social Media",
        "description": "Follow us for updates and announcements:"
      }
    },
    "form": {
      "eyebrow": "Get in Touch",
      "title": "Contact Us",
      "description": "Get in touch with our team and join our community",
      "nameLabel": "What is your name?",
      "emailLabel": "What is your email?",
      "messageLabel": "Message",
      "clearForm": "Clear form",
      "submit": "Submit"
    },
    "getInvolved": {
      "title": "Get Involved",
      "description": "There are multiple ways to participate in our community and activities.",
      "newsletter": {
        "title": "Join our Mailing List",
        "description": "Stay updated with our events, activities, and opportunities by subscribing to our mailing list. We send monthly newsletters and important announcements."
      }
    },
    "faq": {
      "title": "Frequently Asked Questions",
      "items": [
        {
          "question": "Do I need to be a UBA student to participate?",
          "answer": "No! While we're based at UBA, our activities are open to everyone interested in AI safety, regardless of their university affiliation."
        },
        {
          "question": "What background do I need to join your activities?",
          "answer": "It depends on the activity. Some, like our discussion groups, are open to anyone regardless of technical background. Others, like our technical courses, may require programming knowledge or familiarity with machine learning concepts."
        },
        {
          "question": "Are your activities conducted in English or Spanish?",
          "answer": "Most of our activities are conducted in Spanish, but we occasionally have English-language sessions, especially when hosting international speakers. Check the specific event details for language information."
        },
        {
          "question": "How can I start learning about AI safety if I'm completely new to the field?",
          "answer": "We recommend starting with our {resourcesLink}, which has curated materials organized by difficulty level. You can also join our weekly discussion group to learn alongside others in the community."
        },
        {
          "question": "Can I propose a new activity or research direction?",
          "answer": "Absolutely! We're always open to new ideas and collaborations. Please fill in the contact form above with your proposal."
        }
      ]
    }
  },
  "research": {
    "breadcrumb": {
      "home": "Home",
      "current": "Research"
    },
    "title": "Our Research",
    "filterBy": "Filter by:",
    "filters": {
      "all": "All",
      "interpretability": "Interpretability",
      "alignment": "Alignment",
      "robustness": "Robustness",
      "valueLearning": "Value Learning"
    },
    "approachTitle": "Research Approach",
    "focusAreasTitle": "Focus Areas",
    "focusAreas": {
      "mechInterp": "Mechanistic Interpretability - Understanding how neural networks process information internally",
      "alignment": "AI Alignment - Ensuring AI systems act according to human values and intentions",
      "robustness": "Robustness & Adversarial Testing - Building resilient systems resistant to attacks",
      "valueLearning": "Value Learning - Teaching AI systems to learn and respect human preferences"
    },
    "projectsTitle": "Research Projects",
    "linkPlaceholder": "Link Placeholder",
    "publicationsTitle": "Publications",
    "ongoingTitle": "Ongoing Research",
    "started": "Started",
    "expectedCompletion": "Expected Completion"
  },
  "resources": {
    "breadcrumb": {
      "home": "Home",
      "current": "Resources"
    },
    "title": "Learning Resources",
    "description": "Curated materials for exploring AI safety concepts",
    "sections": {
      "featuredVideo": {
        "title": "Introduction to AI Safety",
        "description": "Start here: Why experts fear superintelligent AI – and what we can do about it"
      },
      "selfStudy": {
        "title": "Self-study",
        "lastUpdated": "Last updated: October 22, 2025",
        "description": "These curricula and reading lists enable you to dive deeper into AI safety through independent learning.",
        "fundamentalReading": {
          "title": "Fundamental reading",
          "items": [
            {
              "name": "AI Alignment Forum: Curated Sequences",
              "description": "List of sequences curated by the AI Alignment Forum team, featuring work from Richard Ngo, Paul Christiano, etc.",
              "category": "Technical Alignment",
              "createdBy": "Various",
              "url": "https://www.alignmentforum.org/library"
            }
          ]
        },
        "standardCourses": {
          "title": "Standard introductory courses",
          "items": [
            {
              "name": "BlueDot Impact: Alignment & Governance",
              "description": "Covers key concepts and research perspectives in AI safety, split into two main streams: Alignment and Governance. Previously known as AI Safety Fundamentals.",
              "category": "Technical Alignment, Governance",
              "createdBy": "BlueDot Impact",
              "url": "https://bluedot.org/courses"
            }
          ]
        },
        "relatedResources": {
          "title": "Related resources",
          "eventsTraining": {
            "title": "Events & training",
            "description": "Upcoming fellowships, conferences, facilitated courses etc.",
            "url": "https://www.aisafety.com/events-and-training"
          },
          "aiDigest": {
            "title": "AI Digest",
            "description": "Interactive explainers of AI capabilities and trends",
            "url": "https://theaidigest.org/"
          }
        }
      },
      "externalOpportunities": {
        "title": "External Training Opportunities",
        "subtitle": "Upcoming courses, workshops, and programs from organizations worldwide",
        "description": "There's a wide range of events and training programs in AI safety, both online and in-person. These can help you build skills, make connections, and discover opportunities.",
        "newsletter": {
          "title": "Subscribe to New Opportunities Newsletter",
          "description": "Receive a weekly email summarizing all new events and training programs",
          "cta": "Subscribe"
        }
      }
    }
  },
  "privacyPolicy": {
    "breadcrumb": {
      "home": "Home",
      "current": "Privacy Policy"
    },
    "title": "Privacy Policy",
    "sections": {
      "approach": {
        "title": "Our Approach to Privacy",
        "content": "At BAISH (Buenos Aires AI Safety Hub), we are committed to respecting your privacy. This Privacy Policy outlines our practices regarding the collection, use, and protection of your information when you use our website and services."
      },
      "dataCollection": {
        "title": "Data Collection",
        "content": "We collect minimal personal information. The only personal data we collect is email addresses when users voluntarily sign up for our newsletter through Substack. This information is stored and managed by Substack according to their privacy policy."
      },
      "noTracking": {
        "title": "No Tracking or Cookies",
        "content": "We do not use cookies, analytics, tracking tools, or any other technology to collect data about you. We do not monitor your browsing activities or gather information about your online behaviors."
      },
      "thirdParty": {
        "title": "Third-Party Services",
        "content": "Our newsletter is managed through Substack. When you subscribe to our newsletter, your email address is shared with and stored by Substack. Please refer to",
        "substackLink": "Substack's Privacy Policy",
        "content2": "to understand how they handle your information."
      },
      "rights": {
        "title": "Your Rights",
        "content": "You have the right to unsubscribe from our newsletter at any time by clicking the unsubscribe link in any of our emails or by contacting us directly. If you have any questions about your data or wish to access, correct, or delete your information, please contact us."
      },
      "changes": {
        "title": "Changes to This Policy",
        "content": "We may update this Privacy Policy from time to time. We will notify you of any changes by posting the new Privacy Policy on this page."
      },
      "contact": {
        "title": "Contact Us",
        "content": "If you have any questions about this Privacy Policy, please contact us through our",
        "contactLink": "Contact page"
      }
    }
  }
}
