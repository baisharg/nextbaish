{
  "header": {
    "nav": {
      "about": "About",
      "activities": "Activities",
      "research": "Research",
      "resources": "Resources",
      "contact": "Contact"
    },
    "cta": "Join Us",
    "language": "Language",
    "languages": {
      "en": "English",
      "es": "Español"
    },
    "menu": "Menu",
    "closeMenu": "Close menu",
    "openMenu": "Open menu"
  },
  "footer": {
    "copyright": "All rights reserved.",
    "nav": {
      "about": "About",
      "activities": "Activities",
      "research": "Research",
      "resources": "Resources",
      "contact": "Contact",
      "getInvolved": "Get Involved",
      "privacyPolicy": "Privacy Policy"
    }
  },
  "substack": {
    "eyebrow": "Newsletter",
    "title": "Join our Mailing List",
    "description": "Stay updated with our activities, events, and opportunities.",
    "placeholder": "you@example.com",
    "button": "Subscribe",
    "disclaimer": "We'll only email when something important is happening.",
    "success": "You're subscribed! Check your inbox for the welcome email.",
    "confirm": "Almost done — please confirm the subscription from your inbox.",
    "genericError": "Something went wrong. Please try again.",
    "networkError": "Connection failed. Please check your internet and retry."
  },
  "home": {
    "breadcrumb": {
      "home": "Home"
    },
    "mission": {
      "eyebrow": "Mission",
      "title": "Ensuring AI Benefits Humanity",
      "paragraph1": "As artificial intelligence models advance in capabilities{footnote1}, we expect them to have an increasingly profound impact on our society{footnote2}. It is essential that this impact is positive, and that the decisions made by these systems are transparent, reliable, and accountable{footnote3} to the people affected by them.",
      "paragraph2": "We believe that reducing the risks associated with advanced AI models{footnote4} is one of the most important challenges of our time. We also believe it is an open and exciting problem{footnote5}, with ample opportunities for more researchers to advance in this field{footnote6}.",
      "paragraph3": "BAISH's mission is to support students in entering this field and conducting research on this topic.",
      "cta": "Get Involved"
    },
    "events": {
      "eyebrow": "Upcoming Events",
      "title": "Join our community workshops and meetups",
      "description": "Stay current on AI safety discussions, research sprints, and collaboration spaces.",
      "calendarPlaceholder": "Calendar loads once it's almost in view to keep the initial load quick.",
      "subscribe": "Subscribe to Calendar"
    },
    "activities": {
      "title": "Our Activities",
      "description": "Explore recurring programs designed to grow the AI safety community.",
      "items": {
        "fundamentals": {
          "eyebrow": "Course",
          "title": "AI Safety Fundamentals",
          "description": "Inverted classroom course with weekly discussions on AI alignment and final mini-project.",
          "schedule": "Fridays · 2:30pm",
          "duration": "13 weeks"
        },
        "workshop": {
          "eyebrow": "Workshop",
          "title": "AIS Research Workshop",
          "description": "Technical workshop to replicate recent AI Safety research through programming.",
          "schedule": "Tuesdays · 2:30pm",
          "duration": "2.5 hrs"
        },
        "reading": {
          "eyebrow": "Presentations",
          "title": "Paper Reading Club",
          "description": "Full paper presentations by community members with YouTube recordings.",
          "schedule": "Alternate Thursdays · 3pm",
          "duration": "2 hrs"
        }
      },
      "joinNow": "Join now",
      "learnMore": "Learn more"
    },
    "resources": {
      "title": "Resources",
      "description": "Dive deeper into AI safety with curated readings and tools.",
      "items": {
        "reading": {
          "eyebrow": "Resources",
          "title": "Starter Reading List",
          "description": "Foundational essays and papers to get oriented in AI alignment research.",
          "meta": "15+ papers"
        },
        "toolkit": {
          "eyebrow": "Resources",
          "title": "Research Toolkit",
          "description": "Code repositories, benchmarks, and experiment templates for AI safety research.",
          "meta": "Tools & code"
        },
        "handbook": {
          "eyebrow": "Resources",
          "title": "Community Handbook",
          "description": "Guidelines for hosting meetups and facilitating productive discussions.",
          "meta": "Guide"
        }
      },
      "viewResource": "View resource",
      "learnMore": "Learn more"
    },
    "getInvolved": {
      "communityEyebrow": "Community",
      "communityTitle": "Join our Telegram Community",
      "communityDescription": "Connect with fellow students interested in AI safety.",
      "telegramCta": "Join Telegram Group"
    }
  },
  "about": {
    "breadcrumb": {
      "home": "Home",
      "current": "About AI Safety"
    },
    "title": "Understanding AI Safety",
    "coreConcepts": {
      "whatIsAiSafety": {
        "title": "What is AI Safety?",
        "content": "AI Safety is a research field focused on ensuring that advanced artificial intelligence systems remain beneficial, aligned with human values, and under human control as they become more capable. It encompasses technical research areas like alignment, interpretability, and robustness, as well as governance considerations about how AI systems should be developed and deployed."
      },
      "whyItMatters": {
        "title": "Why It Matters",
        "content": "As AI systems become more powerful and autonomous, they may develop capabilities that could lead to unintended consequences if not properly designed and controlled. The stakes are high: advanced AI could help solve humanity's greatest challenges, but also poses significant risks if developed without adequate safety measures. The field aims to maximize the benefits while minimizing potential harms."
      },
      "risks": {
        "title": "Key Risks & Challenges",
        "alignment": {
          "title": "Alignment Problem",
          "description": "Ensuring AI systems pursue goals aligned with human values and intentions, even as they become more capable."
        },
        "interpretability": {
          "title": "Interpretability",
          "description": "Developing techniques to understand how AI systems make decisions and represent knowledge."
        },
        "robustness": {
          "title": "Robustness",
          "description": "Creating systems that behave safely even when deployed in new environments or facing unexpected situations."
        },
        "powerSeeking": {
          "title": "Power-seeking Behavior",
          "description": "Preventing AI systems from developing instrumental goals that conflict with human welfare."
        },
        "coordination": {
          "title": "Coordination Challenges",
          "description": "Ensuring that safety standards are maintained across all major AI development efforts globally."
        }
      }
    },
    "externalResources": {
      "title": "Learn More About AI Safety",
      "alignmentForum": {
        "title": "Alignment Forum",
        "description": "A forum dedicated to technical research in AI alignment, with papers and discussions from leading researchers.",
        "level": "Technical",
        "cta": "Visit"
      },
      "lessWrong": {
        "title": "LessWrong",
        "description": "A community blog focused on human rationality and the implications of artificial intelligence.",
        "level": "Intermediate",
        "cta": "Visit"
      },
      "eightyK": {
        "title": "80,000 Hours",
        "description": "Career guidance for working on the world's most pressing problems, including AI safety.",
        "level": "Introductory",
        "cta": "Visit"
      },
      "stampy": {
        "title": "Stampy's Wiki",
        "description": "A collaborative wiki providing accessible explanations of AI alignment concepts.",
        "level": "Introductory",
        "cta": "Visit"
      }
    },
    "ourApproach": {
      "title": "Our Approach",
      "focusAreas": {
        "title": "Focus Areas",
        "intro": "At BAISH - Buenos Aires AI Safety Hub, we focus on several key areas within AI safety research:",
        "items": [
          "Mechanistic interpretability of neural networks",
          "Alignment techniques for large language models",
          "Robust training methodologies",
          "Value learning and preference inference"
        ]
      },
      "contribution": {
        "title": "Our Contribution",
        "intro": "We contribute to the field through:",
        "items": [
          "Supporting student research projects",
          "Developing educational resources in Spanish",
          "Building a regional community of AI safety researchers",
          "Organizing workshops and training programs",
          "Mentoring students interested in AI safety careers"
        ]
      }
    },
    "team": {
      "title": "Our Core Team",
      "roles": {
        "coDirector": "Co-founding Director",
        "commDirector": "Communications Director",
        "advisor": "Advisor"
      }
    }
  },
  "activities": {
    "breadcrumb": {
      "home": "Home",
      "current": "Activities"
    },
    "title": "Our Activities",
    "description": "Join our community and participate in AI safety research and learning",
    "subscribeCalendar": "Subscribe to Events Calendar",
    "agiSafety": {
      "title": "AI Safety Fundamentals",
      "status": "Currently Active",
      "description": "The activity consists of discussing and following up in person on the contents of BlueDot's AI Alignment course, which must be completed asynchronously.",
      "overview": "The (unofficial) course uses a flipped classroom format. Each week, participants must come with the content read/watched (2-3 hours), and on Fridays from 2:30 PM to 5:00 PM we meet in person.",
      "whatToExpect": "Course Format",
      "expectations": [
        "13 total sessions: 9 conceptual + 4 project-based",
        "Conceptual sessions: Discussions on content with pre-designed dynamics",
        "Project sessions: Personal mini-project with mentorship",
        "2-3 hours weekly of asynchronous content",
        "2.5 hours weekly in-person (Fridays 2:30-5:00 PM)",
        "Completion certificate (requires 7/9 sessions + final project)"
      ],
      "whoWeSeek": "Who We're Looking For",
      "seekCriteria": [
        "You're interested in addressing concepts like vulnerabilities in AI systems or existential risks",
        "You want to explore what opportunities exist in the field and your next steps",
        "You're willing to participate in open discussions",
        "You have a good level of English",
        "You can dedicate yourself to the in-person course (approximately 2.5 hours per week)",
        "You can dedicate yourself to asynchronous activities and review material (approximately 3 hours per week)"
      ],
      "programDetails": "Program Details",
      "duration": "13 weeks",
      "fellowshipPeriod": "Fridays 2:30-5:00 PM",
      "viewCurriculum": "View Curriculum",
      "applyNow": "Apply Now"
    },
    "aisWorkshop": {
      "title": "AIS Research Workshop",
      "schedule": "Every Tuesday @ 2:30 PM",
      "description": "The activity consists of discussing and replicating (through programming) recent AI Safety research in person to acquire the technical skills necessary to conduct independent research.",
      "overview": "This workshop takes place on Tuesdays from 2:30 PM to 5:00 PM and is designed for students who want to develop practical technical skills in AI safety.",
      "whoWeSeek": "Who We're Looking For",
      "seekCriteria": [
        "You can dedicate at least 5 hours per week (2 hrs in-person + 3 hrs asynchronous)",
        "You have a good level of English",
        "You're willing to participate in open discussions",
        "You're interested in acquiring technical skills in ML and Safety",
        "You want to enter the field and take your first steps as a researcher"
      ],
      "formatTitle": "Workshop Format",
      "format": [
        "2.5-hour in-person sessions on Tuesdays",
        "Discussion of recent AI Safety papers",
        "Practical implementation by programming the discussed methods",
        "Asynchronous work to familiarize yourself with materials (3 hrs/week)",
        "Mentorship to develop independent research skills"
      ],
      "nextSession": "Next Session",
      "date": "Check calendar",
      "time": "2:30 PM - 5:00 PM",
      "location": "Pabellon 0+inf, Ciudad Universitaria",
      "topic": "To be defined weekly",
      "joinTelegram": "Join Telegram for Updates",
      "applyNow": "Apply Now"
    },
    "paperReading": {
      "title": "Paper Reading Club",
      "schedule": "Alternate Thursdays @ 3:00 PM",
      "description": "The Paper Reading Club presents full AI safety papers presented by community members. Each session includes an in-depth presentation followed by discussion.",
      "overview": "Presentations are recorded and uploaded to our YouTube channel so the community can review them later.",
      "selectionCriteriaTitle": "What to Expect",
      "criteria": [
        "Full paper presentations by community members",
        "In-depth analysis of methods and results",
        "Open discussion about implications",
        "Recordings available on YouTube"
      ],
      "sessionFormatTitle": "Session Format",
      "format": [
        "Full paper presentation (30-40 minutes)",
        "Open discussion and Q&A (30-40 minutes)",
        "Recorded session uploaded to YouTube",
        "Telegram chat for coordination and ongoing discussion"
      ],
      "nextSession": "Next Paper Session",
      "paper": "Check Telegram or calendar",
      "discussionLead": "Community member",
      "accessList": "Watch recordings on YouTube",
      "date": "Check calendar",
      "time": "3:00 PM - 5:00 PM",
      "location": "Pabellon 0+inf, Ciudad Universitaria",
      "youtubeChannel": "YouTube Channel",
      "telegramGroup": "Telegram Group"
    },
    "common": {
      "duration": "Duration:",
      "startDate": "Start Date:",
      "endDate": "End Date:",
      "applicationDeadline": "Application Deadline:",
      "location": "Location:",
      "instructors": "Instructors:",
      "fellowshipPeriod": "Fellowship Period:",
      "date": "Date:",
      "time": "Time:",
      "topic": "Topic:",
      "facilitator": "Facilitator:",
      "paper": "Paper:",
      "discussionLead": "Discussion Lead:"
    },
    "gallery": {
      "eyebrow": "Community",
      "title": "Past Events",
      "description": "Moments from our recent gatherings and activities"
    }
  },
  "contact": {
    "breadcrumb": {
      "home": "Home",
      "current": "Contact"
    },
    "title": "Contact Us",
    "description": "Get in touch with our team and join our community",
    "linkText": {
      "resourcesPage": "Resources page"
    },
    "cards": {
      "telegram": {
        "title": "Telegram",
        "description": "Join our community channel for discussions and updates:"
      },
      "location": {
        "title": "Location",
        "description": "We're based at the Department of Computer Science:"
      },
      "social": {
        "title": "Social Media",
        "description": "Follow us for updates and announcements:"
      }
    },
    "form": {
      "title": "Contact Us",
      "description": "Get in touch with our team and join our community",
      "nameLabel": "What is your name?",
      "emailLabel": "What is your email?",
      "messageLabel": "Message",
      "clearForm": "Clear form",
      "submit": "Submit"
    },
    "getInvolved": {
      "title": "Get Involved",
      "description": "There are multiple ways to participate in our community and activities.",
      "newsletter": {
        "title": "Join our Mailing List",
        "description": "Stay updated with our events, activities, and opportunities by subscribing to our mailing list. We send monthly newsletters and important announcements."
      }
    },
    "faq": {
      "title": "Frequently Asked Questions",
      "items": [
        {
          "question": "Do I need to be a UBA student to participate?",
          "answer": "No! While we're based at UBA, our activities are open to everyone interested in AI safety, regardless of their university affiliation."
        },
        {
          "question": "What background do I need to join your activities?",
          "answer": "It depends on the activity. Some, like our discussion groups, are open to anyone regardless of technical background. Others, like our technical courses, may require programming knowledge or familiarity with machine learning concepts."
        },
        {
          "question": "Are your activities conducted in English or Spanish?",
          "answer": "Most of our activities are conducted in Spanish, but we occasionally have English-language sessions, especially when hosting international speakers. Check the specific event details for language information."
        },
        {
          "question": "How can I start learning about AI safety if I'm completely new to the field?",
          "answer": "We recommend starting with our {resourcesLink}, which has curated materials organized by difficulty level. You can also join our weekly discussion group to learn alongside others in the community."
        },
        {
          "question": "Can I propose a new activity or research direction?",
          "answer": "Absolutely! We're always open to new ideas and collaborations. Please fill in the contact form above with your proposal."
        }
      ]
    }
  },
  "research": {
    "breadcrumb": {
      "home": "Home",
      "current": "Research"
    },
    "title": "Our Research",
    "filterBy": "Filter by:",
    "filters": {
      "all": "All",
      "interpretability": "Interpretability",
      "alignment": "Alignment",
      "robustness": "Robustness",
      "valueLearning": "Value Learning"
    },
    "approachTitle": "Research Approach",
    "focusAreasTitle": "Focus Areas",
    "focusAreas": {
      "mechInterp": "Mechanistic Interpretability - Understanding how neural networks process information internally",
      "alignment": "AI Alignment - Ensuring AI systems act according to human values and intentions",
      "robustness": "Robustness & Adversarial Testing - Building resilient systems resistant to attacks",
      "valueLearning": "Value Learning - Teaching AI systems to learn and respect human preferences"
    },
    "projectsTitle": "Research Projects",
    "linkPlaceholder": "Link Placeholder",
    "publicationsTitle": "Publications",
    "ongoingTitle": "Ongoing Research",
    "started": "Started",
    "expectedCompletion": "Expected Completion"
  },
  "resources": {
    "breadcrumb": {
      "home": "Home",
      "current": "Resources"
    },
    "title": "Learning Resources",
    "description": "Curated materials for exploring AI safety concepts with progress tracking",
    "learningPath": {
      "title": "Your Learning Path",
      "description": "Choose your level to see personalized recommendations",
      "beginner": {
        "title": "Beginner",
        "description": "No technical background needed"
      },
      "intermediate": {
        "title": "Intermediate",
        "description": "Some ML/AI knowledge"
      },
      "advanced": {
        "title": "Advanced",
        "description": "Strong technical background"
      },
      "viewing": "Viewing {level} resources",
      "clearFilter": "Clear filter"
    },
    "sections": {
      "featuredVideo": {
        "title": "Introduction to AI Safety",
        "description": "Start here: Why experts fear superintelligent AI – and what we can do about it"
      },
      "quickWins": {
        "title": "Quick Wins",
        "description": "15-30 minute resources for busy learners"
      },
      "communityPicks": {
        "title": "Community Picks",
        "description": "Most recommended by BAISH members"
      },
      "latest": {
        "title": "Latest Additions",
        "description": "Recently added to our collection"
      },
      "externalOpportunities": {
        "title": "External Training Opportunities",
        "subtitle": "Upcoming courses, workshops, and programs from organizations worldwide",
        "description": "There's a wide range of events and training programs in AI safety, both online and in-person. These can help you build skills, make connections, and discover opportunities."
      },
      "allResources": {
        "title": "All Resources",
        "description": "Filter by type, topic, or difficulty level",
        "showing": "Showing {count} of {total} resources"
      }
    },
    "filters": {
      "type": "Type:",
      "topic": "Topic:",
      "all": "All"
    },
    "difficulties": {
      "beginner": "Beginner",
      "intermediate": "Intermediate",
      "advanced": "Advanced"
    },
    "actions": {
      "markComplete": "Mark Complete",
      "completed": "✓ Completed",
      "start": "Start"
    },
    "cta": {
      "title": "Ready to Get Started?",
      "description": "Join our community to discuss these resources and learn together",
      "studyGroups": "Join Study Groups",
      "getInTouch": "Get in Touch"
    }
  },
  "privacyPolicy": {
    "breadcrumb": {
      "home": "Home",
      "current": "Privacy Policy"
    },
    "title": "Privacy Policy",
    "sections": {
      "approach": {
        "title": "Our Approach to Privacy",
        "content": "At BAISH (Buenos Aires AI Safety Hub), we are committed to respecting your privacy. This Privacy Policy outlines our practices regarding the collection, use, and protection of your information when you use our website and services."
      },
      "dataCollection": {
        "title": "Data Collection",
        "content": "We collect minimal personal information. The only personal data we collect is email addresses when users voluntarily sign up for our newsletter through Substack. This information is stored and managed by Substack according to their privacy policy."
      },
      "noTracking": {
        "title": "No Tracking or Cookies",
        "content": "We do not use cookies, analytics, tracking tools, or any other technology to collect data about you. We do not monitor your browsing activities or gather information about your online behaviors."
      },
      "thirdParty": {
        "title": "Third-Party Services",
        "content": "Our newsletter is managed through Substack. When you subscribe to our newsletter, your email address is shared with and stored by Substack. Please refer to",
        "substackLink": "Substack's Privacy Policy",
        "content2": "to understand how they handle your information."
      },
      "rights": {
        "title": "Your Rights",
        "content": "You have the right to unsubscribe from our newsletter at any time by clicking the unsubscribe link in any of our emails or by contacting us directly. If you have any questions about your data or wish to access, correct, or delete your information, please contact us."
      },
      "changes": {
        "title": "Changes to This Policy",
        "content": "We may update this Privacy Policy from time to time. We will notify you of any changes by posting the new Privacy Policy on this page."
      },
      "contact": {
        "title": "Contact Us",
        "content": "If you have any questions about this Privacy Policy, please contact us through our",
        "contactLink": "Contact page"
      }
    }
  }
}
