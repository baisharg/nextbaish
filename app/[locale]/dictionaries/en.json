{
  "header": {
    "nav": {
      "about": "About",
      "activities": "Programs",
      "research": "Research",
      "resources": "Resources",
      "contact": "Contact"
    },
    "cta": "Join Us",
    "language": "Language",
    "languages": {
      "en": "English",
      "es": "Español"
    },
    "menu": "Menu",
    "closeMenu": "Close menu",
    "openMenu": "Open menu"
  },
  "footer": {
    "copyright": "All rights reserved.",
    "nav": {
      "about": "About",
      "activities": "Programs",
      "research": "Research",
      "resources": "Resources",
      "contact": "Contact",
      "getInvolved": "Get Involved",
      "privacyPolicy": "Privacy Policy"
    }
  },
  "substack": {
    "eyebrow": "Newsletter",
    "title": "Join our Mailing List",
    "description": "Stay updated with our activities, events, and opportunities.",
    "placeholder": "you@example.com",
    "button": "Subscribe",
    "disclaimer": "We'll only email when something important is happening.",
    "success": "You're subscribed! Check your inbox for the welcome email.",
    "confirm": "Almost done — please confirm the subscription from your inbox.",
    "genericError": "Something went wrong. Please try again.",
    "networkError": "Connection failed. Please check your internet and retry."
  },
  "home": {
    "breadcrumb": {
      "home": "Home"
    },
    "mission": {
      "eyebrow": "Buenos Aires AI Safety Hub",
      "title": "From Curious to Contributing",
      "tagline": "Supporting your path into AI safety research",
      "paragraph1": "As artificial intelligence models advance in capabilities{footnote1}, we expect them to have an increasingly profound impact on our society{footnote2}. It is essential that this impact is positive, and that the decisions made by these systems are transparent, reliable, and accountable{footnote3} to the people affected by them.",
      "paragraph2": "We believe that reducing the risks associated with advanced AI models{footnote4} is one of the most important challenges of our time. We also believe it is an open and exciting problem{footnote5}, with ample opportunities for more researchers to advance in this field{footnote6}.",
      "paragraph3": "BAISH's mission is to support students in entering this field and conducting research on this topic.",
      "cta": "Get Involved"
    },
    "gettingStarted": {
      "eyebrow": "Your Path",
      "title": "Getting Started with BAISH",
      "description": "Clear steps to go from curious to contributing.",
      "steps": [
        {
          "number": "01",
          "title": "Join our community",
          "description": "Connect with 200+ members on WhatsApp and Telegram. Get updates on events and opportunities.",
          "cta": "Join WhatsApp",
          "link": "https://chat.whatsapp.com/BlgwCkQ8jmpB2ofIxiAi9P"
        },
        {
          "number": "02",
          "title": "Learn the fundamentals",
          "description": "Take our AI Safety Fundamentals course — a 13-week inverted classroom covering core alignment concepts.",
          "cta": "View Course",
          "link": "/activities/fundamentals"
        },
        {
          "number": "03",
          "title": "Build technical skills",
          "description": "Join the AIS Research Workshop to replicate papers and develop research skills hands-on.",
          "cta": "Join Workshop",
          "link": "/activities/workshop"
        },
        {
          "number": "04",
          "title": "Contribute to research",
          "description": "Apply to AISAR for a 6-month research fellowship with mentorship, stipend, and compute access.",
          "cta": "Learn About AISAR",
          "link": "https://aisar.ar"
        }
      ]
    },
    "events": {
      "eyebrow": "Upcoming Events",
      "title": "Join our community workshops and meetups",
      "description": "Stay current on AI safety discussions, research sprints, and collaboration spaces.",
      "calendarPlaceholder": "Calendar loads once it's almost in view to keep the initial load quick.",
      "subscribe": "Subscribe to Calendar"
    },
    "activities": {
      "title": "Our Programs",
      "description": "Explore recurring programs designed to grow the AI safety community.",
      "items": {
        "fundamentals": {
          "eyebrow": "Course",
          "title": "AI Safety Fundamentals",
          "description": "Inverted classroom course with weekly discussions on AI alignment and final mini-project.",
          "schedule": "Fridays · 2:30pm",
          "duration": "13 weeks"
        },
        "workshop": {
          "eyebrow": "Workshop",
          "title": "AIS Research Workshop",
          "description": "Technical workshop to replicate recent AI Safety research through programming.",
          "schedule": "Tuesdays · 2:30pm",
          "duration": "2.5 hrs"
        },
        "reading": {
          "eyebrow": "Reading Group",
          "title": "Paper Presentations Club",
          "description": "Full paper presentations by community members with YouTube recordings.",
          "schedule": "Alternate Thursdays · 3pm",
          "duration": "2 hrs"
        }
      },
      "joinNow": "Join now",
      "joinTelegram": "Join Telegram",
      "learnMore": "Learn more"
    },
    "successStories": {
      "eyebrow": "Impact",
      "title": "Where Our Community Goes",
      "description": "BAISH alumni have gone on to research positions, fellowships, and published work.",
      "stats": [
        { "number": "3+", "label": "Published papers" },
        { "number": "10+", "label": "Research fellowships" },
        { "number": "200+", "label": "Community members" }
      ],
      "stories": [
        {
          "name": "Guido Bergman",
          "role": "AISAR → Algoverse & AI Evals",
          "quote": "BAISH gave me the foundation and connections to transition into AI safety research professionally.",
          "link": "https://www.linkedin.com/in/guido-ernesto-bergman-2251bb203"
        },
        {
          "name": "Ana & Vicky",
          "role": "BAISH → ML4Good Camp",
          "quote": "From local meetups to international research collaborations.",
          "link": null
        }
      ],
      "cta": "See our research",
      "ctaLink": "/research"
    },
    "aisar": {
      "eyebrow": "Sister Project",
      "title": "AISAR Scholarships",
      "description": "In-person part-time program for advanced students to work on AI Safety research projects. $600/month stipend + computing budget. Topics include interpretability, evaluations, and oversight.",
      "duration": "6 months",
      "commitment": "20 hrs/week",
      "visitWebsite": "Visit Website"
    },
    "resources": {
      "title": "Resources",
      "description": "Dive deeper into AI safety with curated readings and tools.",
      "items": {
        "reading": {
          "eyebrow": "Resources",
          "title": "Starter Reading List",
          "description": "Foundational essays and papers to get oriented in AI alignment research.",
          "meta": "15+ papers"
        },
        "toolkit": {
          "eyebrow": "Resources",
          "title": "Research Toolkit",
          "description": "Code repositories, benchmarks, and experiment templates for AI safety research.",
          "meta": "Tools & code"
        },
        "handbook": {
          "eyebrow": "Resources",
          "title": "Community Handbook",
          "description": "Guidelines for hosting meetups and facilitating productive discussions.",
          "meta": "Guide"
        }
      },
      "viewResource": "View resource",
      "learnMore": "Learn more"
    },
    "getInvolved": {
      "communityEyebrow": "Community",
      "communityTitle": "Join our Community",
      "communityDescription": "Connect with fellow students interested in AI safety.",
      "telegramCta": "AI Safety Argentina",
      "telegramMembers": "190+ members",
      "whatsappCta": "BAISH Community",
      "whatsappMembers": "140+ members"
    }
  },
  "about": {
    "breadcrumb": {
      "home": "Home",
      "current": "Team"
    },
    "title": "Our Team",
    "coreConcepts": {
      "whatIsAiSafety": {
        "title": "What is AI Safety?",
        "content": "AI Safety is a research field focused on ensuring that advanced artificial intelligence systems remain beneficial, aligned with human values, and under human control as they become more capable. It encompasses technical research areas like alignment, interpretability, and robustness, as well as governance considerations about how AI systems should be developed and deployed."
      },
      "whyItMatters": {
        "title": "Why It Matters",
        "content": "As AI systems become more powerful and autonomous, they may develop capabilities that could lead to unintended consequences if not properly designed and controlled. The stakes are high: advanced AI could help solve humanity's greatest challenges, but also poses significant risks if developed without adequate safety measures. The field aims to maximize the benefits while minimizing potential harms."
      },
      "risks": {
        "title": "Key Risks & Challenges",
        "alignment": {
          "title": "Alignment Problem",
          "description": "Ensuring AI systems pursue goals aligned with human values and intentions, even as they become more capable."
        },
        "interpretability": {
          "title": "Interpretability",
          "description": "Developing techniques to understand how AI systems make decisions and represent knowledge."
        },
        "robustness": {
          "title": "Robustness",
          "description": "Creating systems that behave safely even when deployed in new environments or facing unexpected situations."
        },
        "powerSeeking": {
          "title": "Power-seeking Behavior",
          "description": "Preventing AI systems from developing instrumental goals that conflict with human welfare."
        },
        "coordination": {
          "title": "Coordination Challenges",
          "description": "Ensuring that safety standards are maintained across all major AI development efforts globally."
        }
      }
    },
    "ourApproach": {
      "title": "Our Approach",
      "focusAreas": {
        "title": "Focus Areas",
        "intro": "At BAISH - Buenos Aires AI Safety Hub, we focus on several key areas within AI safety research:",
        "items": [
          "Chain of Thought interpretability",
          "LLM evaluations",
          "Mechanistic interpretability of neural networks"
        ]
      },
      "contribution": {
        "title": "Our Contribution",
        "intro": "We contribute to the field through:",
        "items": [
          "Supporting student research projects",
          "Building a regional community of AI safety researchers",
          "Organizing workshops, training programs, and hackathons",
          "Mentoring students interested in AI safety careers"
        ]
      }
    },
    "team": {
      "title": "Our Core Team",
      "meetTheTeam": "Meet the Team",
      "cofoundersTitle": "Cofounders",
      "leadershiptTitle": "Leadership",
      "roles": {
        "coDirector": "Co-founding Director",
        "commDirector": "Communications Director",
        "headOfStrategy": "Head of Strategy",
        "advisor": "Advisor"
      },
      "volunteersTitle": "Volunteers",
      "volunteerRoles": {
        "asfFacilitator": "AI Safety Fundamentals Facilitator",
        "aisWorkshopFacilitator": "AIS Workshop Facilitator",
        "programAssistant": "Program Assistant"
      },
      "bios": {
        "eitan": "Eitan is a full-time AI Safety researcher through the {aisarLink} program and the {apartLabLink}. He has co-authored {eitanPaperLink} accepted to NeurIPS workshops, {eitanArxivPaperLink}, and {eitanCoTPaperLink}. He holds a bachelor's + master's degree in Physics from the University of Buenos Aires. He has served as a teaching assistant at {ml4gLink} camps on two occasions and has been a facilitator for the {aisesLink} from the Center for AI Safety. He is currently a facilitator for {blueDotAgiLink}.",
        "luca": "Luca has been involved in AI Safety since 2016. After dropping out of a master's in Computer Science at the University of Buenos Aires, he received grants from {acxLink} and the {ltffLink} to upskill for AI Safety research. He has worked in operations at {nonlinearLink} and achieved first place in two separate {apartResearchLink} sprints and one second place. He currently does part-time operations for the {aiSpeciesLink} YouTube channel, which has garnered over 14 million views raising awareness about AI Safety. He is currently a facilitator for {blueDotAgiLink}."
      }
    },
    "support": {
      "title": "Supported By",
      "description": "BAISH is supported by:",
      "coefficientGiving": {
        "name": "Coefficient Giving",
        "description": "Coefficient Giving's mission is to help others as much as we can with the resources available to us."
      },
      "kairos": {
        "name": "Kairos",
        "program": "Pathfinder Program",
        "description": "Kairos supports AI safety field-building through their Pathfinder program, which helps accelerate promising initiatives."
      },
      "visitWebsiteCta": "Visit website",
      "pathfinderCta": "Pathfinder",
      "kairosCta": "Kairos"
    },
    "callToAction": {
      "title": "Want to chat?",
      "description": "We welcome anyone interested in AI safety to book a call with us!",
      "bookWithEitan": "Book with Eitan",
      "bookWithLuca": "Book with Luca"
    }
  },
  "activities": {
    "breadcrumb": {
      "home": "Home",
      "current": "Programs"
    },
    "title": "Our Programs",
    "description": "Join our community and participate in AI safety research and learning",
    "subscribeCalendar": "Subscribe to Events Calendar",
    "agiSafety": {
      "title": "AI Safety Fundamentals",
      "status": "Currently Active",
      "description": "The activity consists of discussing and following up in person on the contents of BlueDot's AI Alignment course, which must be completed asynchronously.",
      "overview": "The (unofficial) course uses a flipped classroom format. Each week, participants must come with the content read/watched (2-3 hours), and on Fridays from 2:30 PM to 5:00 PM we meet in person.",
      "whatToExpect": "Course Format",
      "expectations": [
        "13 total sessions: 9 conceptual + 4 project-based",
        "Conceptual sessions: Discussions on content with pre-designed dynamics",
        "Project sessions: Personal mini-project with mentorship",
        "2-3 hours weekly of asynchronous content",
        "2.5 hours weekly in-person (Fridays 2:30-5:00 PM)",
        "Completion certificate (requires 7/9 sessions + final project)"
      ],
      "whoWeSeek": "Who We're Looking For",
      "seekCriteria": [
        "You're interested in addressing concepts like vulnerabilities in AI systems or existential risks",
        "You want to explore what opportunities exist in the field and your next steps",
        "You're willing to participate in open discussions",
        "You have a good level of English",
        "You can dedicate yourself to the in-person course (approximately 2.5 hours per week)",
        "You can dedicate yourself to asynchronous activities and review material (approximately 3 hours per week)"
      ],
      "programDetails": "Program Details",
      "duration": "13 weeks",
      "fellowshipPeriod": "Fridays 2:30-5:00 PM",
      "viewCurriculum": "View Curriculum",
      "applyNow": "Apply Now"
    },
    "aisWorkshop": {
      "title": "AIS Research Workshop",
      "schedule": "Every Tuesday @ 2:30 PM",
      "description": "The activity consists of discussing and replicating (through programming) recent AI Safety research in person to acquire the technical skills necessary to conduct independent research.",
      "overview": "This workshop takes place on Tuesdays from 2:30 PM to 5:00 PM and is designed for students who want to develop practical technical skills in AI safety.",
      "whoWeSeek": "Who We're Looking For",
      "seekCriteria": [
        "You can dedicate at least 5 hours per week (2 hrs in-person + 3 hrs asynchronous)",
        "You have a good level of English",
        "You're willing to participate in open discussions",
        "You're interested in acquiring technical skills in ML and Safety",
        "You want to enter the field and take your first steps as a researcher"
      ],
      "formatTitle": "Workshop Format",
      "format": [
        "2.5-hour in-person sessions on Tuesdays",
        "Discussion of recent AI Safety papers",
        "Practical implementation by programming the discussed methods",
        "Asynchronous work to familiarize yourself with materials (3 hrs/week)",
        "Mentorship to develop independent research skills"
      ],
      "nextSession": "Next Session",
      "date": "Check calendar",
      "time": "2:30 PM - 5:00 PM",
      "location": "Pabellon 0+inf, Ciudad Universitaria",
      "topic": "To be defined weekly",
      "joinTelegram": "Join Telegram for Updates",
      "joinWhatsapp": "Join WhatsApp",
      "applyNow": "Apply Now"
    },
    "paperReading": {
      "title": "Paper Presentations Club",
      "schedule": "Alternate Thursdays @ 3:00 PM",
      "description": "The Paper Reading Club presents full AI safety papers presented by community members. Each session includes an in-depth presentation followed by discussion.",
      "overview": "Presentations are recorded and uploaded to our YouTube channel so the community can review them later.",
      "selectionCriteriaTitle": "What to Expect",
      "criteria": [
        "Full paper presentations by community members",
        "In-depth analysis of methods and results",
        "Open discussion about implications",
        "Recordings available on YouTube"
      ],
      "sessionFormatTitle": "Session Format",
      "format": [
        "Full paper presentation (30-40 minutes)",
        "Open discussion and Q&A (30-40 minutes)",
        "Recorded session uploaded to YouTube",
        "Telegram chat for coordination and ongoing discussion"
      ],
      "nextSession": "Next Paper Session",
      "paper": "Check Telegram or calendar",
      "discussionLead": "Community member",
      "accessList": "Watch recordings on YouTube",
      "date": "Check calendar",
      "time": "3:00 PM - 5:00 PM",
      "location": "Pabellon 0+inf, Ciudad Universitaria",
      "youtubeChannel": "YouTube Channel",
      "telegramGroup": "Telegram Group"
    },
    "common": {
      "duration": "Duration:",
      "startDate": "Start Date:",
      "endDate": "End Date:",
      "applicationDeadline": "Application Deadline:",
      "location": "Location:",
      "instructors": "Instructors:",
      "fellowshipPeriod": "Fellowship Period:",
      "date": "Date:",
      "time": "Time:",
      "topic": "Topic:",
      "facilitator": "Facilitator:",
      "paper": "Paper:",
      "discussionLead": "Discussion Lead:"
    },
    "gallery": {
      "eyebrow": "Community",
      "title": "Past Events",
      "description": "Moments from our recent gatherings and activities"
    },
    "pastPrograms": {
      "title": "Past programs",
      "description": "Replay workshops and flagship experiences we've already hosted.",
      "cards": [
        {
          "eyebrow": "Workshop",
          "title": "Agentic Coding Workshop 2025",
          "description": "Hands-on intensive where we explored the BMAD methodology and agent teams to ship production software.",
          "date": "October 3, 2025",
          "location": "BAISH x Y-hat · Rooms 1109 & 1110",
          "slug": "agentic-coding-workshop",
          "cta": "View materials",
          "link": "/agentic-coding-workshop"
        }
      ]
    },
    "sisterProjects": {
      "title": "Sister Projects",
      "description": "Collaborative initiatives advancing AI safety across Latin America"
    },
    "lanais": {
      "eyebrow": "Sister Project",
      "title": "LANAIS",
      "subtitle": "Latin American Network for AI Safety",
      "description": "Connecting researchers and policy-makers for safe artificial intelligence in Latin America. LANAIS maintains a directory of AI safety experts across the region and operates in English, Spanish, and Portuguese.",
      "visitWebsite": "Visit Website"
    },
    "fair": {
      "eyebrow": "Sister Project",
      "title": "FAIR",
      "subtitle": "Frontier Artificial Intelligence Research",
      "description": "AI safety research organization at University of Buenos Aires advancing frontier AI safety through high-impact research. Focus areas include AI alignment, existential risks, governance, and systems evaluations.",
      "visitWebsite": "Visit Website"
    }
  },
  "contact": {
    "breadcrumb": {
      "home": "Home",
      "current": "Contact"
    },
    "title": "Contact Us",
    "description": "Get in touch with our team and join our community",
    "linkText": {
      "resourcesPage": "Resources page"
    },
    "cards": {
      "telegram": {
        "title": "Telegram",
        "description": "Join our community channel for discussions and updates:"
      },
      "location": {
        "eyebrow": "Visit Us",
        "title": "Location",
        "description": "We're based at the Department of Computer Science:"
      },
      "social": {
        "eyebrow": "Social",
        "title": "Social Media",
        "description": "Follow us for updates and announcements:"
      }
    },
    "form": {
      "eyebrow": "Get in Touch",
      "title": "Contact Us",
      "description": "Get in touch with our team and join our community",
      "nameLabel": "What is your name?",
      "emailLabel": "What is your email?",
      "messageLabel": "Message",
      "clearForm": "Clear form",
      "submit": "Submit"
    },
    "getInvolved": {
      "title": "Get Involved",
      "description": "There are multiple ways to participate in our community and activities.",
      "newsletter": {
        "title": "Join our Mailing List",
        "description": "Stay updated with our events, activities, and opportunities by subscribing to our mailing list. We send monthly newsletters and important announcements."
      }
    },
    "faq": {
      "title": "Frequently Asked Questions",
      "items": [
        {
          "question": "Do I need to be a UBA student to participate?",
          "answer": "No! While we're based at UBA, our activities are open to everyone interested in AI safety, regardless of their university affiliation."
        },
        {
          "question": "What background do I need to join your activities?",
          "answer": "It depends on the activity. Some, like our discussion groups, are open to anyone regardless of technical background. Others, like our technical courses, may require programming knowledge or familiarity with machine learning concepts."
        },
        {
          "question": "Are your activities conducted in English or Spanish?",
          "answer": "Most of our activities are conducted in Spanish, but we occasionally have English-language sessions, especially when hosting international speakers. Check the specific event details for language information."
        },
        {
          "question": "How can I start learning about AI safety if I'm completely new to the field?",
          "answer": "We recommend starting with our {resourcesLink}, which has curated materials organized by difficulty level. You can also join our weekly discussion group to learn alongside others in the community."
        },
        {
          "question": "Can I propose a new activity or research direction?",
          "answer": "Absolutely! We're always open to new ideas and collaborations. Please fill in the contact form above with your proposal."
        }
      ]
    }
  },
  "research": {
    "breadcrumb": {
      "home": "Home",
      "current": "Research"
    },
    "title": "Research at BAISH",
    "intro": "We help students in Buenos Aires go from curious to published. Join research sprints, workshops, and a community of aspiring AI safety researchers.",
    "ctaPrograms": "Join a Program",
    "ctaPublications": "View Publications",
    "ctaLearnMore": "Learn More",
    "pathway": {
      "title": "Your Research Journey",
      "subtitle": "From first steps to published researcher — here's how it works",
      "steps": [
        {
          "number": "01",
          "title": "Learn",
          "program": "AI Safety Fundamentals",
          "description": "Build foundations through our 13-week course covering alignment, interpretability, and AI governance.",
          "duration": "13 weeks",
          "link": "/activities/fundamentals",
          "icon": "book"
        },
        {
          "number": "02",
          "title": "Practice",
          "program": "AIS Research Workshop",
          "description": "Replicate recent AI safety papers through hands-on programming. Develop the technical skills to conduct independent research.",
          "duration": "Ongoing",
          "link": "/activities/workshop",
          "icon": "code"
        },
        {
          "number": "03",
          "title": "Research",
          "program": "Research Sprints",
          "description": "Contribute original work in collaborative sprints. Past participants have placed in Apart Research competitions.",
          "duration": "2-4 weeks",
          "link": "/contact",
          "icon": "lightbulb"
        },
        {
          "number": "04",
          "title": "Launch",
          "program": "AISAR & Careers",
          "description": "Advance to full-time research through AISAR scholarships, independent projects, or industry positions.",
          "duration": "6 months+",
          "link": "https://scholarship.aisafety.ar/",
          "icon": "rocket",
          "external": true
        }
      ]
    },
    "focusAreas": {
      "title": "What You Could Work On",
      "subtitle": "Current research directions in our community",
      "areas": [
        {
          "title": "Mechanistic Interpretability",
          "description": "Understanding how neural networks process information internally. What circuits implement specific behaviors? How can we reverse-engineer model cognition?",
          "icon": "microscope"
        },
        {
          "title": "LLM Evaluations",
          "description": "Building benchmarks and testing methodologies for frontier models. How do we measure alignment? What capabilities emerge at scale?",
          "icon": "chart"
        },
        {
          "title": "Alignment Theory",
          "description": "Fundamental questions about making AI systems beneficial. How do we specify human values? What oversight mechanisms work?",
          "icon": "compass"
        }
      ]
    },
    "publications": {
      "title": "Community Publications",
      "subtitle": "Work by researchers connected to BAISH",
      "items": [
        {
          "title": "Table Top Agents",
          "authors": "Luca De Leo",
          "venue": "Apart Research Hackathon · Nov 2025",
          "description": "AI-powered framework that accelerates AI governance scenario exploration through autonomous agent tabletop exercises, compressing preparation cycles from years to minutes.",
          "links": [
            {
              "label": "Apart",
              "url": "https://apartresearch.com/project/table-top-agents-i2zx"
            }
          ]
        },
        {
          "title": "Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity",
          "authors": "Austin Meek, Eitan Sprejer, Iván Arcuschin, Austin J. Brockmeier, Steven Basart",
          "venue": "arXiv · Oct 2025",
          "description": "Investigating how well chain-of-thought reasoning can be monitored for safety through faithfulness and verbosity metrics.",
          "links": [
            {
              "label": "arXiv",
              "url": "https://arxiv.org/abs/2510.27378"
            }
          ]
        },
        {
          "title": "AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs",
          "authors": "María Victoria Carro, Denise Mester, Facundo Nieto, Oscar Stanchi, Guido Bergman, Mario Leiva, Eitan Sprejer, Luca Forziati Gangi, et al.",
          "venue": "arXiv · Oct 2025",
          "description": "Studying how AI systems' internal beliefs affect their persuasiveness in debate scenarios — implications for AI safety and deception.",
          "links": [
            {
              "label": "arXiv",
              "url": "https://arxiv.org/abs/2510.13912"
            }
          ]
        },
        {
          "title": "Approximating Human Preferences Using a Multi-Judge Learned System",
          "authors": "Eitan Sprejer, Fernando Avalos, Augusto Mariano Bernardi, José Pedro Brito de Azevedo Faustino, Jacob Haimes, Narmeen Fatimah Oozeer",
          "venue": "NeurIPS Workshop · Sep 2025",
          "description": "A multi-judge approach to better approximate human preferences in AI systems, improving alignment evaluation.",
          "links": [
            {
              "label": "arXiv",
              "url": "https://arxiv.org/abs/2510.25884"
            }
          ]
        },
        {
          "title": "RobustCBRN Eval: A Practical Benchmark Robustification Toolkit",
          "authors": "Luca De Leo, James Sykes, Balázs László, Ewura Ama Etruwaa Sam",
          "venue": "Apart Research Hackathon · Sep 2025",
          "award": "2nd Place",
          "description": "A pipeline addressing CBRN evaluation vulnerabilities through consensus detection, verified cloze scoring, and statistical evaluation with bootstrap confidence intervals.",
          "links": [
            {
              "label": "Apart",
              "url": "https://apartresearch.com/project/robustcbrn-eval-a-practical-benchmark-robustification-toolkit-6m2t"
            }
          ]
        },
        {
          "title": "Four Paths to Failure: Red Teaming ASI Governance",
          "authors": "Luca De Leo, Zoé Roy-Stang, Heramb Podar, Damin Curtis, Vishakha Agrawal, Ben Smyth",
          "venue": "Apart Research Hackathon · Jun 2025",
          "award": "1st Place",
          "description": "Stress-tested A Narrow Path Phase 0 ASI moratorium, identifying four circumvention routes and proposing ten mutually reinforcing policy amendments.",
          "links": [
            {
              "label": "Apart",
              "url": "https://apartresearch.com/project/four-paths-to-failure-red-teaming-asi-governance-se53"
            }
          ]
        }
      ],
      "note": "Our community continues to grow — we're building our publication track record through programs like AISAR and Apart Research sprints."
    },
    "expressInterest": {
      "eyebrow": "Get Involved",
      "title": "Express Interest in Research",
      "description": "Want to contribute to AI safety research? Let us know your background and interests, and we'll connect you with relevant projects and collaborators.",
      "formLabel": "Use our contact form to tell us about your background and research interests.",
      "cta": "Contact Us",
      "link": "/contact",
      "note": "We review messages regularly and reach out when there's a good fit."
    },
    "cta": {
      "title": "Ready to start your research journey?",
      "description": "Book a call with one of our co-founders to discuss your interests and find the right path.",
      "bookWithEitan": "Book with Eitan",
      "bookWithLuca": "Book with Luca",
      "eitanSpecialty": "Interpretability & Evaluations",
      "lucaSpecialty": "Operations & Strategy"
    }
  },
  "resources": {
    "breadcrumb": {
      "home": "Home",
      "current": "Resources"
    },
    "title": "Learning Resources",
    "description": "Curated materials for exploring AI safety concepts",
    "sections": {
      "featuredVideo": {
        "title": "Introduction to AI Safety",
        "description": "Start here: Why experts fear superintelligent AI – and what we can do about it"
      },
      "selfStudy": {
        "title": "Self-study",
        "lastUpdated": "Last updated: October 22, 2025",
        "description": "These curricula and reading lists enable you to dive deeper into AI safety through independent learning.",
        "fundamentalReading": {
          "title": "Fundamental reading",
          "items": [
            {
              "name": "AI Alignment Forum: Curated Sequences",
              "description": "List of sequences curated by the AI Alignment Forum team, featuring work from Richard Ngo, Paul Christiano, etc.",
              "category": "Technical Alignment",
              "createdBy": "Various",
              "url": "https://www.alignmentforum.org/library"
            }
          ]
        },
        "standardCourses": {
          "title": "Standard introductory courses",
          "items": [
            {
              "name": "BlueDot Impact: Alignment & Governance",
              "description": "Covers key concepts and research perspectives in AI safety, split into two main streams: Alignment and Governance. Previously known as AI Safety Fundamentals.",
              "category": "Technical Alignment, Governance",
              "createdBy": "BlueDot Impact",
              "url": "https://bluedot.org/courses"
            }
          ]
        },
        "relatedResources": {
          "title": "Related resources",
          "eventsTraining": {
            "title": "Events & training",
            "description": "Upcoming fellowships, conferences, facilitated courses etc.",
            "url": "https://www.aisafety.com/events-and-training"
          },
          "aiDigest": {
            "title": "AI Digest",
            "description": "Interactive explainers of AI capabilities and trends",
            "url": "https://theaidigest.org/"
          },
          "agenticCoding": {
            "title": "Agentic Coding Workshop",
            "description": "BMAD methodology, agents, workflows, and tools for AI-assisted development",
            "url": "/agentic-coding-workshop"
          }
        }
      },
      "externalOpportunities": {
        "title": "External Training Opportunities",
        "subtitle": "Upcoming courses, workshops, and programs from organizations worldwide",
        "description": "There's a wide range of events and training programs in AI safety, both online and in-person. These can help you build skills, make connections, and discover opportunities.",
        "newsletter": {
          "title": "Subscribe to New Opportunities Newsletter",
          "description": "Receive a weekly email summarizing all new events and training programs",
          "cta": "Subscribe"
        },
        "timeline": {
          "title": "Open Applications Timeline",
          "loading": "Loading timeline..."
        }
      }
    }
  },
  "privacyPolicy": {
    "breadcrumb": {
      "home": "Home",
      "current": "Privacy Policy"
    },
    "title": "Privacy Policy",
    "sections": {
      "approach": {
        "title": "Our Approach to Privacy",
        "content": "At BAISH (Buenos Aires AI Safety Hub), we are committed to respecting your privacy. This Privacy Policy outlines our practices regarding the collection, use, and protection of your information when you use our website and services."
      },
      "dataCollection": {
        "title": "Data Collection",
        "content": "We collect minimal personal information. The only personal data we collect is email addresses when users voluntarily sign up for our newsletter through Substack. This information is stored and managed by Substack according to their privacy policy."
      },
      "noTracking": {
        "title": "No Tracking or Cookies",
        "content": "We do not use cookies, analytics, tracking tools, or any other technology to collect data about you. We do not monitor your browsing activities or gather information about your online behaviors."
      },
      "thirdParty": {
        "title": "Third-Party Services",
        "content": "Our newsletter is managed through Substack. When you subscribe to our newsletter, your email address is shared with and stored by Substack. Please refer to",
        "substackLink": "Substack's Privacy Policy",
        "content2": "to understand how they handle your information."
      },
      "rights": {
        "title": "Your Rights",
        "content": "You have the right to unsubscribe from our newsletter at any time by clicking the unsubscribe link in any of our emails or by contacting us directly. If you have any questions about your data or wish to access, correct, or delete your information, please contact us."
      },
      "changes": {
        "title": "Changes to This Policy",
        "content": "We may update this Privacy Policy from time to time. We will notify you of any changes by posting the new Privacy Policy on this page."
      },
      "contact": {
        "title": "Contact Us",
        "content": "If you have any questions about this Privacy Policy, please contact us through our",
        "contactLink": "Contact page"
      }
    }
  },
  "agenticCodingWorkshop": {
    "metadata": {
      "title": "Agentic Coding Workshop — BAISH",
      "description": "All the materials from the Agentic Coding workshop: BMAD method, agents, workflows, and resources to get started immediately.",
      "openGraph": {
        "title": "Agentic Coding Workshop — BAISH",
        "description": "Complete agentic coding (BMAD) methodology with agents, workflows, and practical resources."
      },
      "twitter": {
        "title": "Agentic Coding Workshop — BAISH",
        "description": "Complete agentic coding (BMAD) methodology with agents, workflows, and practical resources."
      }
    },
    "nav": {
      "overview": "Overview",
      "problem": "Problem",
      "solution": "Solution",
      "agents": "Agents",
      "planning": "Planning",
      "development": "Development",
      "resources": "Resources"
    },
    "breadcrumb": {
      "home": "Home",
      "current": "Agentic Coding Workshop"
    },
    "hero": {
      "title": "Agentic Coding Workshop",
      "tagline": "Build projects with AI as if you had a full team: BMAD methodology, agents, processes, and tools ready to use.",
      "eventDetails": "Friday, October 3, 2025 · 4:00 PM · Rooms 1109 & 1110 — BAISH x Y-hat",
      "note": "This page brings together every material and resource from the workshop.",
      "whatsapp": "Join the \"Agentic Coding\" WhatsApp group inside the BAISH community to discuss ideas, share projects, and get help with BMAD.",
      "buttons": {
        "startNow": "Start now",
        "baish": "BAISH",
        "yHat": "Y-hat",
        "joinWhatsapp": "Join the WhatsApp",
        "viewResources": "See resources"
      }
    },
    "problem": {
      "eyebrow": "Context",
      "title": "The problem with traditional LLM workflows",
      "description": "Two obstacles make \"give the model everything\" fail: long-context degradation and the absence of real memory over time.",
      "sourcePrefix": "Source:",
      "cards": {
        "contextDegradation": {
          "title": "Long-context degradation",
          "description": "The LoCoDiff benchmark (January 2025) shows that even the best available model, Sonnet 4.5, degrades sharply with long contexts.",
          "highlight": "What happens: accuracy drops from 96% with 2K–21K-token contexts to 64% beyond 60K tokens. When you hand the whole codebase to the model, it drowns in noise.",
          "details": [
            "State-of-the-art models still rely on manageable context windows to keep high accuracy.",
            "Feeding entire files or repositories produces noise, repetition, and inconsistent decisions."
          ],
          "sourceLabel": "LoCoDiff Benchmark"
        },
        "lackOfLongTermMemory": {
          "title": "No long-term memory",
          "description": "A METR study (July 2025) found that experienced developers working on their own projects were 20% slower using AI than working alone.",
          "highlight": "Why? The LLM restarts from scratch every time: it lacks the tacit context that humans accumulate. Experts predicted they would be 39% faster, but reality was the opposite.",
          "details": [
            "Observed time per story: 1.67 h without AI vs. 2.26 h with AI assistance.",
            "Expectations forecasted 24%–39% faster delivery, exposing the gap between predictions and reality."
          ],
          "sourceLabel": "METR AI R&D Study (July 2025)"
        }
      }
    },
    "solution": {
      "eyebrow": "BMAD method",
      "title": "The solution: an agentic team that plans and executes",
      "description": "BMAD is an AI development method where eight specialized agents work like a real team. They plan the whole project before coding and then execute story by story with continuous validation.",
      "description2": "Each agent is the same LLM with a specialized prompt and verified, sharded context. No magic memory—just structured documentation within reach.",
      "calloutTitle": "Why BMAD works",
      "cards": {
        "sharding": {
          "title": "Sharding",
          "description": "Large documents are split into focused shards. Instead of handing the Developer a 10,000-token PRD, they receive a 400–600-token shard with only what matters for the story.",
          "outcome": "Result: every agent operates in the 96% accuracy zone (<5K tokens) without extra noise."
        },
        "specializedAgents": {
          "title": "Specialized agents",
          "description": "Eight agents with concrete roles: Product Manager, Architect, Developer, QA, etc. Each has tailored instructions and checklists, and only queries the context they need.",
          "outcome": "Result: surgically precise context for every decision without overloading the model."
        },
        "structuredDocumentation": {
          "title": "Structured documentation",
          "description": "Rigorous planning before coding: complete PRD, defined architecture, and sequential stories that validate each other. That creates the \"memory\" LLMs don't have.",
          "outcome": "Result: agents work with professional, verifiable information always aligned with the project goal."
        }
      },
      "reasons": {
        "optimizedContext": {
          "title": "1. Optimized context",
          "description": "In the METR study, teams worked with huge codebases in Cursor and ended up 20% slower. BMAD does the opposite: PM handles 2K tokens of requirements, Architecture ~3K tokens, and the Developer a single epic (1.5K tokens). High accuracy, no degradation."
        },
        "validationTransparency": {
          "title": "2. Validation and transparency",
          "description": "The Product Owner validates every story draft before development, QA reviews the Developer's work, and humans approve each milestone. Artifacts stay human-readable and versioned in the repo."
        },
        "advancedElicitation": {
          "title": "3. Advanced elicitation",
          "description": "Agents ask questions and iterate on documentation. It's not just generating code—they discover, clarify, and update requirements while keeping the person in the loop."
        }
      }
    },
    "agentsSection": {
      "eyebrow": "Agentic team",
      "title": "The eight BMAD agents",
      "description": "Each role is the same model with a specialized prompt and sharded context. The outcome: a multidisciplinary team that works like a human crew.",
      "artifactsLabel": "Artifacts"
    },
    "agents": {
      "businessAnalyst": {
        "name": "Business Analyst",
        "role": "Insightful Analyst & Strategic Ideation Partner",
        "description": "Market research, brainstorming, competitive analysis, project briefs, and brownfield documentation.",
        "artifacts": [
          "Project brief",
          "Market research",
          "Competitor analysis",
          "Brainstorming output"
        ]
      },
      "productManager": {
        "name": "Product Manager",
        "role": "Investigative Product Strategist & Market-Savvy PM",
        "description": "Creates PRDs, defines product strategy, prioritizes features, and keeps stakeholders aligned. Mandatory step in the method.",
        "artifacts": ["PRD (Product Requirements Document)", "Brownfield PRD"]
      },
      "architect": {
        "name": "Architect",
        "role": "Holistic System Architect & Full-Stack Technical Leader",
        "description": "System design, technical architecture, tech selection, API contracts, and infrastructure plans. Mandatory step in the method.",
        "artifacts": [
          "Full-stack architecture",
          "Backend architecture",
          "Frontend architecture",
          "Brownfield architecture"
        ]
      },
      "uxExpert": {
        "name": "UX Expert",
        "role": "User Experience Designer & UI Specialist",
        "description": "UI/UX design, wireframes, prototypes, front-end specifications, and ready-to-use prompts for v0 or Lovable.",
        "artifacts": ["Front-end spec", "UI design prompts"]
      },
      "productOwner": {
        "name": "Product Owner",
        "role": "Technical Product Owner & Process Steward",
        "description": "Manages the backlog, refines stories, defines acceptance criteria, and keeps artifacts coherent. Critical: sharding prevents model degradation.",
        "artifacts": ["Sharded documents", "Epic files", "Story validations"]
      },
      "scrumMaster": {
        "name": "Scrum Master",
        "role": "Technical Scrum Master & Story Preparation Specialist",
        "description": "Prepares clear stories, manages epics, runs retros, and keeps the agile cadence. Produces drafts the Developer can implement without friction.",
        "artifacts": [
          "User story drafts",
          "Sequential tasks",
          "Acceptance criteria"
        ]
      },
      "developer": {
        "name": "Developer",
        "role": "Expert Senior Software Engineer & Implementation Specialist",
        "description": "Implements production code, covers tests, refactors, and documents decisions. Works story by story with test coverage.",
        "artifacts": [
          "Production code",
          "Unit tests",
          "Integration tests",
          "E2E tests",
          "Code documentation"
        ]
      },
      "qa": {
        "name": "QA",
        "role": "Test Architect with Quality Advisory Authority",
        "description": "Defines testing strategies, profiles risks, ensures requirement traceability, and issues the final quality decision.",
        "artifacts": [
          "QA results & gate decisions",
          "Risk profiles",
          "Test plans & design",
          "Requirements tracing",
          "NFR assessments"
        ]
      }
    },
    "commands": {
      "eyebrow": "Operational flow",
      "title": "How the commands work",
      "description": "Agents are invoked from the terminal or IDE with short commands. You always pass context via @filename.md.",
      "syntax": {
        "title": "Command syntax",
        "description": "Pick your tool and follow the same structure: command + action + relevant files.",
        "blocks": {
          "claudeOpencode": {
            "heading": "Claude Code / OpenCode"
          },
          "cursorWindsurf": {
            "heading": "Cursor / Windsurf"
          }
        }
      },
      "switch": {
        "title": "Switch agents without noise",
        "description": "Each agent runs in its own context. To swap cleanly:",
        "steps": [
          "Clear the context with /clear (Claude Code) or /new (OpenCode).",
          "Invoke the agent with its init command (e.g., /po *shard-doc).",
          "Pass only the relevant files using @filename.md."
        ],
        "tip": {
          "prefix": "Tip:",
          "beforeDocs": "Keep your documents in",
          "beforeExample": "and use clear filenames (e.g.,",
          "afterExample": ") so you shard context precisely."
        }
      }
    },
    "planning": {
      "eyebrow": "Phase 1",
      "title": "Planning (one-time upfront)",
      "description": "Complete design before writing code. Everything runs from the terminal with BMAD commands.",
      "steps": {
        "pmPrdCreation": {
          "title": "PM: PRD Creation",
          "summary": "The Product Manager creates the Product Requirements Document, guiding the user section by section.",
          "bullets": [
            "Interactive process: the PM asks questions and records answers on the fly.",
            "Defines features and epics (only story titles at this stage).",
            "Captures functional and non-functional requirements.",
            "Prioritizes MVP vs. roadmap and logs key dependencies.",
            "Each section is reviewed and approved before moving on."
          ],
          "callout": {
            "title": "Important note",
            "text": "Stories in the PRD stay brief (titles only). Later the Scrum Master expands them with detailed tasks."
          }
        },
        "architectDesign": {
          "title": "Architect: System Design",
          "summary": "The Architect reads the PRD and designs the full technical architecture, always in an interactive flow.",
          "bullets": [
            "Chooses tech stack for frontend, backend, and database.",
            "Defines folder structure and code organization.",
            "Designs APIs and contracts between components.",
            "Covers scalability, security, observability, and performance.",
            "Each section is validated with the user before advancing."
          ]
        },
        "poChecklist": {
          "title": "PO: Master Checklist",
          "summary": "The Product Owner ensures PRD and architecture are perfectly aligned before sharding.",
          "bullets": [
            "Checks coherence between requirements and technical design.",
            "Confirms the architecture covers every prioritized feature.",
            "Identifies gaps, contradictions, or risks that must be resolved first."
          ],
          "callout": {
            "title": "If something doesn’t align",
            "text": "Documents keep iterating until everything makes sense. No moving forward without full validation."
          }
        },
        "poSharding": {
          "title": "PO: Sharding",
          "summary": "The Product Owner shards the PRD and architecture into manageable epics and stories (<2K tokens).",
          "bullets": [
            "Runs the terminal program that splits the documents automatically.",
            "Produces small shards inside docs/epics and docs/stories.",
            "Each file holds focused context and is saved as versionable Markdown.",
            "Example: a large PRD becomes docs/epics/epic-1-auth.md, docs/epics/epic-2-dashboard.md, etc."
          ],
          "callout": {
            "title": "Outcome",
            "text": "Prioritized backlog with sharded stories ready for iterative development."
          }
        }
      },
      "optionalAgentsTitle": "Optional agents when needed",
      "optionalAgents": [
        "Business Analyst: greenfield research and initial project brief.",
        "UX Expert: interface specs and prompts for tools like v0 or Lovable.",
        "QA: profiles risks and quality criteria before development starts."
      ],
      "summaryTitle": "When planning ends you have:",
      "summaryPoints": [
        "Full PRD, reviewed and approved.",
        "Architecture defined and aligned with the PRD.",
        "Sharded, prioritized story backlog.",
        "Structured, versioned context for the entire team."
      ],
      "summaryNote": "You haven’t written a single line of code yet. That intentional approach prevents weeks of refactors later on."
    },
    "development": {
      "eyebrow": "Phase 2",
      "title": "Iterative development loop",
      "description": "Story-by-story implementation with built-in validation. Repeat this loop until the backlog is done.",
      "steps": {
        "smReviewNotes": {
          "title": "SM: Review previous notes",
          "summary": "The Scrum Master reviews notes from the last story to start with accumulated learning.",
          "bullets": [
            "Identifies what worked and what didn’t.",
            "Recovers important technical decisions.",
            "Reviews feedback from Developer and QA.",
            "Documents takeaways for the next story."
          ],
          "callouts": [
            {
              "title": "Learning loop",
              "text": "Today’s notes feed tomorrow’s draft. The team improves story after story."
            }
          ]
        },
        "smDraftNextStory": {
          "title": "SM: Draft next story",
          "summary": "The Scrum Master drafts the next story using only the relevant context.",
          "bullets": [
            "Reads only the matching epic (not the whole PRD).",
            "Reviews architecture and PRD just enough.",
            "Produces very clear sequential tasks.",
            "Defines specific acceptance criteria.",
            "Includes enough context without drowning the model."
          ]
        },
        "poValidateDraft": {
          "title": "PO: Validate story draft",
          "summary": "The Product Owner validates the draft against the PRD before coding begins.",
          "bullets": [
            "Checks that the draft aligns with the original objectives.",
            "Confirms every requirement is covered.",
            "Flags gaps or contradictions for immediate correction.",
            "Can add recommendations for the Scrum Master."
          ],
          "callouts": [
            {
              "title": "Alignment check",
              "text": "Ensures the Developer implements exactly what was agreed during planning."
            }
          ]
        },
        "devImplementation": {
          "title": "Dev: Implementation",
          "summary": "The Developer implements the story following architecture, UX, and the testing checklist.",
          "bullets": [
            "Ships production-ready code aligned with the architecture.",
            "Writes unit, integration, and E2E tests as needed.",
            "Covers error handling and edge cases.",
            "Can invoke MCPs like Playwright to automate end-to-end tests."
          ]
        },
        "qaReview": {
          "title": "QA: Test story thoroughly",
          "summary": "QA acts as the quality gatekeeper before the story advances.",
          "bullets": [
            "Runs all tests (unit, integration, E2E).",
            "Performs manual testing across user flows and edge cases.",
            "Verifies acceptance criteria are met.",
            "Profiles risks (security, performance, reliability).",
            "Issues verdict: PASS (all good) / CONCERNS (review) / FAIL (critical) / WAIVED (explicitly accepted)."
          ],
          "callouts": [
            {
              "title": "Quality gates",
              "text": "PASS (all good) • CONCERNS (minor flags) • FAIL (critical issue) • WAIVED (explicitly accepted risk)."
            },
            {
              "title": "Critical check",
              "text": "QA ensures the code works and meets professional standards before moving forward."
            }
          ]
        },
        "devFixes": {
          "title": "Dev: Fix according to QA review",
          "summary": "The Developer addresses the QA report and brings everything to PASS status.",
          "bullets": [
            "Reviews the QA outcome (PASS/CONCERNS/FAIL).",
            "Implements targeted fixes and improvements.",
            "Resolves every CONCERN and FAIL logged.",
            "Reruns tests to confirm nothing regresses."
          ],
          "callouts": [
            {
              "title": "Iterative",
              "text": "If QA issues a FAIL, the review repeats until the result is PASS or WAIVED."
            }
          ]
        },
        "markDone": {
          "title": "Mark done & next story",
          "summary": "Close the story and loop back to step one with the next priority.",
          "bullets": [
            "Update the backlog and mark the story as done.",
            "Pick the next story from the prioritized backlog.",
            "Return to step one: SM reviews the notes that were just written."
          ],
          "callouts": [
            {
              "title": "Iterative loop",
              "text": "Story after story, commit after commit, until the project is complete."
            }
          ]
        }
      },
      "loopReminder": "Repeat the loop: go back to step one with the next story.",
      "notesTitle": "Process notes",
      "notes": [
        "Integrated validation: PO signs off before coding, QA tests afterward, Developer fixes and documents.",
        "Quality flexibility: accept CONCERNS for an MVP or require PASS for critical software.",
        "Continuous learning loop: Developer notes feed the SM in the next iteration."
      ],
      "advantagesTitle": "Benefits of the iterative loop",
      "advantagesPoints": [
        "You always finish each story with a functional, tested version.",
        "You can pivot quickly: every cycle delivers independent value.",
        "Learning accumulates through notes and short retros.",
        "The person stays in control, approving every key transition."
      ]
    },
    "resources": {
      "eyebrow": "Resources",
      "title": "Everything to start today",
      "description": "Tools, templates, and guides to run BMAD without friction.",
      "toolingTitle": "Which tool should you use?",
      "toolOptions": {
        "claudeCode": {
          "title": "Claude Code",
          "highlight": "Best option: Anthropic's official tool with Sonnet 4.5 (top of the benchmark).",
          "bullets": [
            "Highest quality",
            "Seamless integration",
            "$100/mo (Max Plan)"
          ]
        },
        "openCode": {
          "title": "OpenCode",
          "highlight": "Most versatile: sign in with your provider and it uses that plan.",
          "bullets": [
            "Login with Anthropic, GitHub Copilot, Z.ai, etc.",
            "Uses your provider’s plan",
            "Open source and multi-model"
          ],
          "note": "Requires a workaround (details below)."
        },
        "cursorWindsurf": {
          "title": "Cursor / Windsurf",
          "highlight": "Popular IDEs that support BMAD with @mention syntax.",
          "bullets": [
            "Full-featured editors",
            "Large community",
            "@agent syntax"
          ]
        },
        "geminiCli": {
          "title": "Gemini CLI",
          "highlight": "Free: Gemini 2.5 Pro with official BMAD support.",
          "bullets": [
            "Gemini 2.5 Pro",
            "Free included usage",
            "Official BMAD support"
          ]
        },
        "droidCli": {
          "title": "Droid CLI",
          "highlight": "Free: GPT-5-Codex via Factory AI.",
          "bullets": ["GPT-5-Codex model", "100% free", "Factory AI platform"],
          "note": "Requires a workaround (details below)."
        }
      },
      "quickStart": {
        "title": "Workshop repo — start now",
        "description": "Ready-to-go repository with BMAD preinstalled, setup scripts, and configured MCPs.",
        "bullets": [
          "BMAD preinstalled and configured",
          "Setup script for Droid CLI (free)",
          "Includes MCPs (Sequential Thinking, Playwright)",
          "BMAD commands preloaded",
          "Example workflows"
        ],
        "linkLabel": "github.com/baisharg/Workshop-Vibe-Coding",
        "note": "The script installs Droid CLI (free, GPT-5-Codex) and configures all BMAD commands automatically.",
        "calloutLabel": "Quick start"
      },
      "learningTitle": "Repositories and learning",
      "learning": {
        "bmadRepository": {
          "title": "BMAD Repository",
          "description": "Complete repo with prompts, documentation, and examples.",
          "bullets": [
            "Prompts for all eight agents",
            "Automated setup",
            "Full documentation",
            "Project examples"
          ],
          "linkLabel": "github.com/bmad-code-org/bmad-method"
        },
        "bmadMasterclass": {
          "title": "BMAD Method Masterclass",
          "description": "Video tutorial that walks the method end to end.",
          "bullets": [
            "Step-by-step setup",
            "How to use each agent",
            "Full workflow",
            "Live examples"
          ],
          "linkLabel": "youtu.be/LorEJPrALcg"
        }
      },
      "plansTitle": "Recommended plans",
      "plans": {
        "zaiPlan": {
          "title": "Z.ai Coding Plan",
          "highlight": "Recommended: affordable choice for students and makers.",
          "bullets": [
            "Only $3/mo",
            "GLM 4.6 model",
            "Performance close to Sonnet 4",
            "Ideal for students"
          ],
          "linkLabel": "z.ai/subscribe"
        },
        "claudeMax": {
          "title": "Claude Code + Max Plan",
          "highlight": "For enthusiasts: highest quality for agentic coding.",
          "bullets": [
            "Anthropic's official Terminal Agent",
            "Access to Sonnet 4.5",
            "Max Plan: $100/mo with higher limits",
            "Optimized agent experience"
          ],
          "linkLabel": "claude.com/product/claude-code"
        },
        "githubStudent": {
          "title": "GitHub Student Pack",
          "highlight": "Students get Copilot Pro for free.",
          "bullets": [
            "Copilot Pro included",
            "Access to Sonnet 4.5",
            "Top model on the benchmark",
            "No cost for students"
          ],
          "linkLabel": "education.github.com/pack"
        }
      },
      "cliTitle": "CLI / Terminal tools",
      "cliInstallLabel": "Installation:",
      "cliSetupLabel": "BMAD configuration:",
      "cli": {
        "openCode": {
          "title": "OpenCode",
          "description": "Open-source terminal program. Sign in with your provider and OpenCode uses that plan.",
          "bullets": [
            "Login with Anthropic, Copilot, Z.ai, etc.",
            "Runs in any IDE terminal",
            "MCP server compatible",
            "Multi-model and open source"
          ],
          "linkLabel": "opencode.ai",
          "steps": [
            "Install BMAD and pick the “Claude Code” option.",
            "Rename the .claude/ directory to .opencode/.",
            "Move the .md files to the top level (not inside agents/ or tasks/)."
          ],
          "note": "Needs a small workaround to reuse the prompts."
        },
        "droidCli": {
          "title": "Droid CLI",
          "description": "Factory AI client that exposes GPT-5-Codex for free, fully compatible with BMAD.",
          "bullets": [
            "GPT-5-Codex model",
            "Completely free",
            "Agent-oriented terminal program"
          ],
          "linkLabel": "docs.factory.ai/cli",
          "steps": [
            "Install BMAD and pick the “Claude Code” option.",
            "Rename .claude/ to .factory/.",
            "Move the .md files to the top level of .factory/."
          ],
          "note": "Same workaround as OpenCode to reuse the prompts."
        }
      },
      "setupTitle": "Installation & setup",
      "setup": {
        "installBmad": {
          "title": "Install BMAD",
          "description": "BMAD installs per project at the repo root, keeping everything versioned.",
          "commandLabel": "At your project root:",
          "bullets": [
            "Creates the .bmad-core/ folder with agents and templates.",
            "Per-project install (not global).",
            "Everything stays under version control."
          ]
        },
        "recommendedMcps": {
          "title": "Recommended MCP tools",
          "description": "Tools that expand what agents can do.",
          "bullets": [
            "Playwright: browser automation for E2E testing.",
            "Sequential Thinking: structured reasoning.",
            "Explore more MCPs in the Smithery repository."
          ],
          "linkLabel": "smithery.ai — MCP tools repository",
          "note": "Configure MCPs in your IDE and agents will use them when needed."
        }
      }
    }
  }
}
